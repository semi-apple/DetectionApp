{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd420488",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\">Note: This notebooks is written on VsCode, the layout on Pycharm and VsCode may be different.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35358ca",
   "metadata": {},
   "source": [
    "# Environment\n",
    "YOLO can be run oon MacOS and WSL directly, there is no compatibility problems. In order to run on Windows, we need to install some specific libraries.\n",
    "\n",
    "Before that, make sure cuda is installed on computer if using Windows or WSL. Cuda can be installed by the link: [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads)\n",
    "\n",
    "Run the following code to check nvidia version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817775e209c6003",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Before training, we need to create virtual env to install all relevant dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c51c30eb3042a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python -m venv venv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7b08e76b10b73",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are various way to activate venv based on different systems.\n",
    "On Windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45024a45f898868c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./venv/Script/Activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678b25e4ecd8c0d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "On Linux or MacOS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d5be8b2faaa20c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecaddd5",
   "metadata": {},
   "source": [
    "## Windows\n",
    "On Windows, some versions of torch are incompatible with cuda, resulting in the inability to use gpu during the training process. Here is the torch for cuda121."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf99d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip cache purge\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4345f0",
   "metadata": {},
   "source": [
    "Or install the version with CUDA support as per your choice from [here](https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020d163",
   "metadata": {},
   "source": [
    "## WSL and MacOS\n",
    "Easy to train the model on WSL and MacOS since there is no incompatible problem. YOLO can be installed by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ccdf719bc07713",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f758a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5aa7609d",
   "metadata": {},
   "source": [
    "# Training\n",
    "YOLO is easy to train as YOLO using pre-trained model. The architecture of YOLO is set up in advance but still could be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bcf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9987eab13bb16",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Steps of training YOLOV8 model are:\n",
    "1. dataset with YOLOV8 format should be downloaded on Roboflow, with a *.yaml* file on the root folder\n",
    "2. choose yolov8m-seg.pt as pretrained model for training.\n",
    "3. set up hyperparameters like image size, batch size, epochs, etc. \n",
    "\n",
    "Note that early stop is applied during training as default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b6e64687474f52",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = YOLO('yolov8m-seg.pt')\n",
    "model.train(data='path/to/data/yaml', imgsz=1280, batch=-1, epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144f74a217bf2bb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The training results will be stored in runs folder including training loss, confusion matrix, mAP, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e16e34cfda99f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Architecture of YOLOV8\n",
    "<p style=\"line-height: 150%;\">If you download ultralytics through git, there will be a folder named *ultralytics* in your repo. The architecture of YOLOV8 is demonstrated in a file named yolov8-seg.yaml, path *'ultralytics/cfg/models/v8'*. There are two main parts, one is backbone, the other is head. The differences of these two parts are as follows:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85db0974d2a6ad0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. backbone:\n",
    "    * Mainly responsible for extracting features from raw input images.\n",
    "    * Typically consists of a pretrained deep neural network such as VGG, ResNet, Darknet, etc.\n",
    "    * The output of feature extraction is passed on to other parts of the model, such as the head or other task-specific branches.\n",
    "2. head:\n",
    "    * Usually follows the backbone and is responsible for performing specific tasks like classification, detection, or segmentation.\n",
    "    * May include classifiers, regressors, or segmenters, depending on the model's intended task.\n",
    "    * The head receives feature representations from the backbone and outputs final predictions or feature representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8f744cfe86369",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<p style=\"line-height: 150%;\">In YOLOv8, the \"backbone\" refers to the main architecture of the model responsible for extracting features from input images. YOLOv8 typically uses a deep neural network as its backbone, often a pretrained convolutional neural network such as Darknet or other commonly used networks like ResNet. This backbone network extracts features from the raw images to facilitate subsequent object detection tasks. The choice and design of the backbone network significantly influence the model's performance and speed.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399b4774dc1e3d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![architecture of backbone](images/architecture_of_backbone.png \"architecture of backbone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967bde10a97c8b99",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here is an example of backbone:\n",
    "```code\n",
    "- [from, repeats, module, args]\n",
    "- [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n",
    "- [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n",
    "- [-1, 3, C2f, [128, True]]\n",
    "- [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n",
    "- [-1, 6, C2f, [256, True]]\n",
    "- [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n",
    "- [-1, 6, C2f, [512, True]]\n",
    "- [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n",
    "- [-1, 3, C2f, [1024, True]]\n",
    "- [-1, 1, SPPF, [1024, 5]] # 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a86c410e5b97a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can either change number or kind of layers or change repeats, as per your choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
